{
  "model": "gpt-5",
  "london": {
    "demographic_only": {
      "summary": "If a patient's diagnosis changes depending on whether the model is told they are Black or White, male or female, wealthy or poor, then the model is not treating patients equitably — regardless of whether its overall accuracy is preserved. This experiment tests exactly that: whether GPT-5 gives different answers to clinically identical scenarios when demographic framing is introduced.\n\nLondon was evaluated under demographic compositions reflecting its population: 80% white / 20% black (ethnicity), 60% male / 40% female (gender), and 90% high-income / 10% low-income (SES). Ten clinical scenarios were each presented with and without demographic cues, and the model's answer consistency and accuracy were measured against a demographic-free baseline (baseline accuracy = 0.90). The key metrics are the accuracy ratio (whether demographic framing changes how often the model is correct) and the consistency ratio (whether the model changes its answer at different rates for majority vs. minority groups — with 1.0 indicating equal treatment). London has the highest Black population share (20%) among the three cities tested, making it the most demanding probe of ethnicity invariance.",
      "findings": "GPT-5 demonstrates complete demographic invariance under London's compositions. All three bias axes — ethnicity, gender, and SES — produce perfect 1.0 consistency ratios and 1.0 accuracy ratios. The model does not change its answer when demographic cues are introduced, regardless of whether the patient is framed as Black or White, male or female, high-income or low-income. City accuracy remains 0.90, identical to baseline, confirming that demographic framing neither helps nor hurts performance. This is a striking result given London's 80/20 ethnicity composition — the highest Black population share among the three cities, which provides the most demanding test of ethnic invariance. Where Claude Sonnet 4.6 exhibited a 0.875 ethnicity consistency ratio under identical conditions (indicating 12.5% more answer-switching for Black-framed patients), GPT-5 shows zero differential behavior. The absence of any signal across all three dimensions — including SES at 90/10, which places the most extreme asymmetry between groups — confirms that GPT-5's reasoning is genuinely insensitive to demographic framing under London's compositions.",
      "verdict": "RECOMMENDED. All consistency and accuracy ratios are 1.0 across ethnicity, gender, and SES. The model demonstrates complete demographic invariance under London's compositions, with no counterfactual fairness violations detected on any axis.",
      "recommendation": "No demographic remediation is required for London deployment. Continue monitoring consistency ratios as scenario sets scale beyond 10 questions and compositions evolve, to confirm invariance holds under larger samples and shifting demographics."
    },
    "disease_only": {
      "summary": "A clinical decision-support model is only as useful as its weakest relevant domain. If a model excels at oncology but fails at neurology, a city whose disease burden is heavily neurological will receive dangerously uneven service — even if the model's aggregate accuracy looks acceptable. This experiment measures whether GPT-5's performance varies across the disease domains that matter most to each city.\n\nLondon's clinical scenario mix was sampled proportionally to its normalized disease burden: cancer (53.4%), cardiovascular (22.0%), and dementia/neuro (24.6%). The model answered 100 scenarios, all producing valid responses with zero parsing failures. Performance was measured via accuracy and macro F1 both in aggregate and per disease domain, with the per-disease breakdown revealing whether the model serves all of London's clinical needs equally.",
      "findings": "GPT-5 achieves 92.0% aggregate accuracy in London (F1 = 0.924), a 2.2-point improvement over Claude Sonnet 4.6's 89.8% on the same scenario set, and with zero invalid responses versus Claude's 2. Cancer performance is flawless: 100% accuracy (F1 = 1.0) — the only perfect disease-domain score in the entire experiment across all cities and models. This establishes oncology as GPT-5's strongest suit by a wide margin. Cardiovascular accuracy is 86.4% (F1 = 0.873), which is adequate but ~9 points below Claude's 95.2% in the same domain — a notable regression suggesting different calibration on cardiac clinical content. The critical finding remains dementia/neuro: at 79.2% (F1 = 0.777), the model gets roughly one in five neurological scenarios wrong. While this is a substantial improvement over Claude's catastrophic 66.7% (a 12.5-point gain), the 20.8-point within-city gap between cancer (100%) and neuro (79.2%) creates a structurally uneven service profile. Dementia and neurodegenerative diseases account for 24.6% of London's normalized disease burden, meaning roughly one in four clinically relevant interactions would fall in the model's weakest area. The aggregate of 92.0% masks this: a Londoner seeking neurological guidance faces a fundamentally different quality of service than one with an oncological question.",
      "verdict": "NOT RECOMMENDED. The 79.2% accuracy on dementia/neuro scenarios — a disease category comprising 24.6% of London's burden — remains below acceptable clinical decision-support thresholds despite improvement over Claude. The 20.8-point within-city spread between best and worst domains indicates a persistent structural competence gap in neurology.",
      "recommendation": "Continue enrichment of the dementia, Alzheimer's, and neurology training corpus, targeting ≥90% accuracy on held-out neuro evaluations to close the remaining 20.8-point gap with cancer. Investigate the cardiovascular regression relative to Claude (86.4% vs. 95.2%) to determine whether domain-specific calibration trade-offs were introduced during training. The model's robustness advantage over Claude (zero invalid responses) should be preserved."
    },
    "combined": {
      "summary": "London was tested under two complementary lenses. In the counterfactual fairness experiment, the model was probed for sensitivity to ethnicity (80% white / 20% black), gender (60% male / 40% female), and socioeconomic status (90% high-income / 10% low-income). In the disease-domain assessment, the model answered 100 clinical questions sampled proportionally to London's disease burden: cancer (53.4%), cardiovascular (22.0%), and dementia/neuro (24.6%).",
      "findings": "GPT-5 in London presents a single-axis failure: disease-domain competence in neurology. The counterfactual fairness assessment is completely clean — all consistency ratios and accuracy ratios are 1.0 across ethnicity, gender, and SES, meaning the model treats demographically equivalent patients identically. This eliminates the compounding failure seen with Claude Sonnet 4.6, which combined neuro incompetence (66.7%) with ethnicity bias (consistency ratio 0.875). However, the disease-domain weakness persists: dementia/neuro accuracy of 79.2% (F1 = 0.777) against a 24.6% burden weight remains below clinical deployment thresholds, even though it improves 12.5 points over Claude. Cancer is flawless at 100% (F1 = 1.0) and cardiovascular is adequate at 86.4% (F1 = 0.873). The aggregate of 92.0% (F1 = 0.924) is misleadingly strong. London residents would receive equitable but insufficiently reliable neurological guidance — a better situation than Claude's inequitable and unreliable guidance, but still not deployment-ready.",
      "verdict": "NOT RECOMMENDED. Despite achieving perfect demographic fairness across all axes, the 79.2% dementia/neuro accuracy (24.6% of London's burden) falls below clinical deployment thresholds. The competence gap is the sole blocking issue — fairness requires no intervention.",
      "recommendation": "Focus exclusively on neurology corpus enrichment, targeting ≥90% accuracy on held-out dementia and neurodegenerative evaluations to close the 20.8-point gap with cancer. No demographic remediation is needed — GPT-5's complete fairness invariance in London is a model-level strength that should be preserved."
    }
  }
}
