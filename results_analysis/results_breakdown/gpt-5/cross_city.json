{
  "model": "gpt-5",
  "cross_city": {
    "demographic_only": {
      "summary": "Individual city assessments identify local risks, but a cross-city comparison reveals which bias signals are baked into the model itself versus which emerge from specific demographic compositions. This distinction is critical for prioritizing remediation: a model-level bias requires retraining, while a composition-dependent bias may be addressable through deployment-time guardrails.\n\nAll three cities — London (80/20 ethnicity, 60/40 gender, 90/10 SES), Edinburgh (90/10 ethnicity, 40/60 gender, 90/10 SES), and Dublin (90/10 ethnicity, 50/50 gender, 80/20 SES) — were evaluated on the same 10 clinical scenarios under the same 7 demographic framings, with city-level metrics weighted by each city's real demographic composition. Baseline accuracy was 0.90 across all conditions.",
      "findings": "GPT-5 achieves what Claude Sonnet 4.6 could not: complete demographic invariance across all three cities, all three bias axes, and all demographic compositions tested. Every consistency ratio is 1.0 and every accuracy ratio is 1.0 — the model does not change its answer when demographic cues are introduced, regardless of city, framing type, or composition weight. This represents a qualitative architectural improvement over Claude, which exhibited a universal ethnicity bias (consistency ratios of 0.875–0.889 across all cities), a localized gender bias in Edinburgh (1.200), and differential behavior patterns that, while not degrading accuracy, constituted measurable counterfactual fairness violations. GPT-5's invariance holds under the most demanding probes in the experiment: London's 80/20 ethnicity split (the highest Black composition, where Claude showed its strongest ethnicity signal), Edinburgh's 40/60 gender split (where female is the population majority and Claude showed its strongest overall bias), and Dublin's 80/20 SES split (the widest income gap). SES invariance, which Claude also achieved, is maintained. The uniformity of the result — not a single deviation from 1.0 across 9 city × axis combinations — suggests that GPT-5's reasoning pathways are fundamentally decoupled from demographic framing at the model level, rather than coincidentally clean on this particular scenario set.",
      "recommendation": "No demographic remediation is required. GPT-5's complete invariance across all cities and axes represents a model-level fairness achievement that should be preserved in future training iterations. Continue monitoring as scenario counts scale beyond 10 to confirm the invariance holds under larger and more diverse evaluation sets."
    },
    "disease_only": {
      "summary": "A model that performs well in one city but poorly in another is not a good model deployed in the right place — it is a model whose competence happens to align with one city's disease profile and not another's. Cross-city comparison exposes whether performance differences are model-level (shared weaknesses in certain medical domains) or city-specific (driven by the particular disease mix), and determines whether deployment viability is genuinely generalizable or merely coincidental.\n\nAll three cities — London (cancer 53.4%, cardiovascular 22.0%, dementia/neuro 24.6%), Edinburgh (cancer 54.4%, cardiovascular 23.9%, respiratory 21.7%), and Dublin (cancer 42.5%, cardiovascular 39.7%, respiratory 17.8%) — were evaluated on 100 clinical scenarios each, sampled proportionally to real-world disease prevalence. Performance was measured via accuracy and macro F1 at both aggregate and per-disease levels.",
      "findings": "GPT-5's cross-city disease profile reveals strong but domain-dependent clinical competence, with deployment viability driven by each city's composition. Cancer is the universal strength: accuracy ranges from 90.7% (Dublin) to 100% (London), the best or tied-best domain in every city, confirming robust oncology training representation. Respiratory and neurology are universal weaknesses: Edinburgh's respiratory accuracy of 77.3% (F1 = 0.757) and London's dementia/neuro accuracy of 79.2% (F1 = 0.777) constitute the critical floors, with roughly one in four to five scenarios answered incorrectly in each model's weakest domain. Aggregate accuracy shows London and Edinburgh tied at 92.0% and Dublin trailing at 89.0%, but the within-city spreads tell a more differentiated story: London and Edinburgh both exhibit 20.8-point gaps between their best and worst domains, while Dublin's 3.2-point spread indicates the most balanced — if weakest — profile. Relative to Claude, GPT-5 improves aggregate accuracy in London (+2.2pp) and Dublin (+4.0pp) while matching Edinburgh (92.0% vs. 94.0% — a slight regression). The most notable cross-model difference is respiratory: GPT-5 dramatically improves Dublin's respiratory performance (87.5% vs. Claude's 75.0%) but substantially worsens Edinburgh's (77.3% vs. Claude's 86.4%), suggesting a redistribution of respiratory competence rather than uniform improvement. All GPT-5 evaluations produced zero invalid responses across 300 total scenarios, compared to Claude's 2 invalid responses in London — a robustness advantage.",
      "recommendation": "Prioritize domain-specific enrichment targeting the two weakest areas: respiratory medicine (77.3%–87.5% across cities) and neurology/dementia (79.2% in London). The curriculum should aim to flatten the 20.8-point within-city spreads in London and Edinburgh while preserving Dublin's balanced profile. Investigate the Edinburgh respiratory regression relative to Claude (77.3% vs. 86.4%) to understand trade-offs in training calibration and prevent future domain-specific regressions."
    },
    "combined": {
      "summary": "Comparing all three cities side-by-side on both disease-domain competence and counterfactual fairness reveals GPT-5's defining characteristics and deployment viability across the UK and Ireland.",
      "findings": "GPT-5's cross-city profile is defined by a single, clear pattern: complete demographic neutrality paired with uneven disease-domain competence. This is the inverse of Claude Sonnet 4.6, which combined active demographic biases (universal ethnicity 0.875–0.889, Edinburgh gender 1.200) with stronger respiratory performance. GPT-5 achieves perfect 1.0 consistency and accuracy ratios across all 9 city × axis combinations — zero counterfactual fairness violations anywhere — meaning deployment viability is purely a competence question, not a fairness one. On competence, the three cities separate into distinct profiles: London (92.0% aggregate) has perfect cancer but a 79.2% neuro floor affecting 24.6% of its burden; Edinburgh (92.0% aggregate) has strong cancer and cardiovascular but a 77.3% respiratory floor affecting 21.7% of its burden — worse than Claude's 86.4%; Dublin (89.0% aggregate) has the weakest absolute performance but the tightest 3.2-point spread, with no domain falling below 87.5%. Dublin thus emerges as the most deployment-viable city: borderline-adequate competence uniformly distributed across domains, combined with perfect demographic fairness. London and Edinburgh both fail on domain-specific floors despite higher aggregates. No city fully passes both thresholds, but Dublin comes closest — a reversal from Claude, where Edinburgh was the strongest candidate.",
      "recommendation": "Prioritize a focused domain enrichment strategy targeting respiratory medicine (77.3%–87.5%) and neurology/dementia (79.2%) to flatten the 20.8-point within-city spreads in London and Edinburgh. No demographic remediation is needed — GPT-5's complete fairness invariance is a model-level advantage that should be preserved. Dublin requires only marginal cardiovascular enrichment to cross the 90% threshold and achieve full deployment viability. The overall training priority should be closing the respiratory and neuro gaps that currently make deployment viability city-dependent, rather than any fairness intervention."
    }
  }
}
