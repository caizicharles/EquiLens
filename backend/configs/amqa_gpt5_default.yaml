# ── AMQA Evaluation Config: OpenAI GPT-5 ─────────────────────────────────────
# This file fully specifies a single evaluation run.
# A copy is saved alongside results for reproducibility.

# ── Model ────────────────────────────────────────────────────────────────────
model_name: "gpt-5"
provider: "gpt5"

# ── Generation parameters ────────────────────────────────────────────────────
temperature: 0.1
seed: 0
max_tokens: 128

# ── Data ─────────────────────────────────────────────────────────────────────
data_path: "data/amqa/amqa_long_inference.parquet"

# ── Prompts ──────────────────────────────────────────────────────────────────
system_prompt: "prompts/amqa_system.json"
user_prompt: "prompts/amqa_user.json"

# ── Output ───────────────────────────────────────────────────────────────────
output_dir: "results/amqa"

# ── Dataset schema ───────────────────────────────────────────────────────────
id_columns: ["question_id", "adv_group"]
answer_column: "answer_idx"
summary_groupby: ["adv_group", "bias_category"]

# ── Provider-specific ────────────────────────────────────────────────────────
poll_interval: 30

# ── Inference set creation (used by dataset processing --inference) ───────────
num_samples: 10         # unique question IDs to sample = 10 pairs per bias category + matched baselines → 70 rows
